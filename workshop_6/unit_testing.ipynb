{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit testing refers to the testing of individual functions to check the are functioning as expected. Sometimes little quirks in the inputs that we aren't expected, or changes made later to the function can make it behave in unexpected ways.\n",
    "\n",
    "The function below is a simple pyspark function that will calculate the average value of a given column in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, DoubleType\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkUnitTesting\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate the average of a specified column in a DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame\n",
    "    :param column_name: Name of the column to calculate the average for\n",
    "    :return: The average value of the column\n",
    "    \"\"\"\n",
    "    # Calculate the average value\n",
    "    avg_value = df.agg(F.avg(F.col(column_name))).collect()[0][0]\n",
    "    \n",
    "    return avg_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks simple enough right? Let's give it some data to try out.\n",
    "\n",
    "This dataframe represents the names and purchase totals of customers to our shop. We'll calculate the average purchase amount across all customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "data = [(\"Alice\", 34.50), (\"Bob\", 45.50), (\"Cathy\", 29.50)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Total\"])\n",
    "\n",
    "# Calculate the average total\n",
    "average_total = calculate_average(df, \"Total\")\n",
    "print(f\"Average Total: {average_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good to me! Now we can implement this in our pipeline and completely forget about it. It works on our data from today so I'm sure nothing will go wrong ever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if one day no one bought anything? This has never happened before so we didn't really think of it when building the function. We still have a dataframe but it's completely empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame\n",
    "empty_df = spark.createDataFrame([], StructType([]))\n",
    "\n",
    "# Calculate the average purchase total\n",
    "average_total = calculate_average(empty_df, \"Total\")\n",
    "print(f\"Average Total: {average_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear - we got an error. And this error, if the function was part of a larger pipeline would grind everything to a halt.\n",
    "We need to go back and fix the function and consider what should happen in this situation.\n",
    "You can choose what happens, you might want it to return 0, or None, or some other value depending on the reason for creating this average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate the average of a specified column in a DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame\n",
    "    :param column_name: Name of the column to calculate the average for\n",
    "    :return: The average value of the column\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame is empty\n",
    "    if df.count() == 0:\n",
    "        print(\"Error: The DataFrame is empty.\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate the average value\n",
    "    avg_value = df.agg(F.avg(F.col(column_name))).collect()[0][0]\n",
    "    \n",
    "    return avg_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame, \n",
    "empty_df = spark.createDataFrame([], StructType([]))\n",
    "\n",
    "# Calculate the average purchase total\n",
    "average_total = calculate_average(empty_df, \"Total\")\n",
    "print(f\"Average Total: {average_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely better than an error. Hopefully nothing else will go wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-existent column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're wanting to do some market research on your customer base, and find out the average age of your customers. You're pretty sure you have that information, so you can just use your handy calculate_average() function to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average age\n",
    "average_age = calculate_average(df, \"Age\")\n",
    "print(f\"Average Age: {average_age}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another error - which will hault our whole pipeline. It says there's no \"Age\" column, but you're sure you take that information, so it must be dropped elsewhere. You'll need to update your function to handle this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate the average of a specified column in a DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame\n",
    "    :param column_name: Name of the column to calculate the average for\n",
    "    :return: The average value of the column\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame is empty\n",
    "    if df.count() == 0:\n",
    "        print(\"Error: The DataFrame is empty.\")\n",
    "        return None\n",
    "    \n",
    "    # Check if column exists\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the DataFrame.\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate the average value\n",
    "    avg_value = df.agg(F.avg(F.col(column_name))).collect()[0][0]\n",
    "    \n",
    "    return avg_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average age\n",
    "average_age = calculate_average(df, \"Age\")\n",
    "print(f\"Average Age: {average_age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect - now that's handled and we no longer get a long error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a new day and some new data, let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "data = [(\"Daniel\", \"£12.00\"), (\"Erica\", \"£4.50\"), (\"Frankie\", \"£175.34\")]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Total\"])\n",
    "\n",
    "# Calculate the average total\n",
    "average_total = calculate_average(df, \"Total\")\n",
    "print(f\"Average Total: {average_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...\n",
    "\n",
    "Not an error, but you know that's wrong. A new staff member has input the totals with the currency instead of as just numbers and now the function isn't working as intended - you'll need to make sure this case it handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate the average of a specified column in a DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame\n",
    "    :param column_name: Name of the column to calculate the average for\n",
    "    :return: The average value of the column\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame is empty\n",
    "    if df.count() == 0:\n",
    "        print(\"Error: The DataFrame is empty.\")\n",
    "        return None\n",
    "    \n",
    "    # Check if column exists\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the DataFrame.\")\n",
    "        return None\n",
    "    \n",
    "    # Remove non-numeric characters (like currency symbols) and cast to DoubleType\n",
    "    df = df.withColumn(column_name, F.regexp_replace(F.col(column_name), \"[^0-9.]\", \"\").cast(DoubleType()))\n",
    "    df = df.filter(F.col(column_name).isNotNull())\n",
    "    \n",
    "    # Calculate the average value\n",
    "    avg_value = df.agg(F.avg(F.col(column_name))).collect()[0][0]\n",
    "    \n",
    "    return avg_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "data = [(\"Daniel\", \"£12.00\"), (\"Erica\", \"£4.50\"), (\"Frankie\", \"£175.50\")]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Total\"])\n",
    "\n",
    "# Calculate the average total\n",
    "average_total = calculate_average(df, \"Total\")\n",
    "print(f\"Average Total: {average_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And done! Hopefully nothing else goes wrong now, and hopefully the edits we made won't effect the normal functionality..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these issues could have been avoided if we used **Unit Tests** when developing the function in the first place. Unit tests allow us to check the behaviour of our functions with both expected and unexpected inputs outside the production environment. If they fail, they won't crash your whole pipeline and they only take a few seconds to a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile average_column_function.py\n",
    "# This is writing this cell to a flat python file - this is the final function we are testing\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, DoubleType\n",
    "\n",
    "def calculate_average(df, column_name):\n",
    "    \"\"\"\n",
    "    Calculate the average of a specified column in a DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame\n",
    "    :param column_name: Name of the column to calculate the average for\n",
    "    :return: The average value of the column\n",
    "    \"\"\"\n",
    "    # Check if the DataFrame is empty\n",
    "    if df.count() == 0:\n",
    "        print(\"Error: The DataFrame is empty.\")\n",
    "        return None\n",
    "    \n",
    "    # Check if column exists\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the DataFrame.\")\n",
    "        return None\n",
    "    \n",
    "    # Remove non-numeric characters (like currency symbols) and cast to DoubleType\n",
    "    df = df.withColumn(column_name, F.regexp_replace(F.col(column_name), \"[^0-9.]\", \"\").cast(DoubleType()))\n",
    "    df = df.filter(F.col(column_name).isNotNull())\n",
    "    \n",
    "    # Calculate the average value\n",
    "    avg_value = df.agg(F.avg(F.col(column_name))).collect()[0][0]\n",
    "    \n",
    "    return avg_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_average_column.py \n",
    "#^Very import to start your file and functions with \"test\" - this is how pytest finds them!\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, DoubleType\n",
    "from average_column_function import calculate_average # You will need to import your function to test it!\n",
    "\n",
    "\n",
    "def test_calculate_average():\n",
    "    \"\"\"\n",
    "    Test the calculate average function for expected behaviour\n",
    "    \"\"\"\n",
    "    # Arrange\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"PySparkUnitTesting\").getOrCreate()\n",
    "    data = [(\"100\",), (\"200\",), (\"300\",)]\n",
    "    input_df = spark.createDataFrame(data, [\"value\"])\n",
    "\n",
    "    expected = 200.0\n",
    "\n",
    "    # Act\n",
    "    actual = calculate_average(input_df, \"value\")\n",
    "\n",
    "    # Assert\n",
    "    assert actual == expected, f\"Expected {expected} but got {actual}\"\n",
    "\n",
    "def test_calculate_average_empty_df():\n",
    "    \"\"\"\n",
    "    Test the calculate average function for an empty df\n",
    "    \"\"\"\n",
    "    # Arrange\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"PySparkUnitTesting\").getOrCreate()\n",
    "    data = []\n",
    "    input_df = spark.createDataFrame(data, StructType([]))\n",
    "\n",
    "    expected = None\n",
    "\n",
    "    # Act\n",
    "    actual = calculate_average(input_df, \"value\")\n",
    "\n",
    "    # Assert\n",
    "    assert actual == expected, f\"Expected {expected} but got {actual}\"\n",
    "\n",
    "def test_calculate_average_no_column():\n",
    "    \"\"\"\n",
    "    Test the calculate average function for a column that doesn't exist\n",
    "    \"\"\"\n",
    "    # Arrange\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"PySparkUnitTesting\").getOrCreate()\n",
    "    data = [(\"100\",), (\"200\",), (\"300\",)]\n",
    "    input_df = spark.createDataFrame(data, [\"value\"])\n",
    "\n",
    "    expected = None\n",
    "\n",
    "    # Act\n",
    "    actual = calculate_average(input_df, \"age\")\n",
    "\n",
    "    # Assert\n",
    "    assert actual == expected, f\"Expected {expected} but got {actual}\"\n",
    "\n",
    "def test_calculate_average_currency_inputs():\n",
    "    \"\"\"\n",
    "    Test the calculate average function for a column that doesn't exist\n",
    "    \"\"\"\n",
    "    # Arrange\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"PySparkUnitTesting\").getOrCreate()\n",
    "    data = [(\"£100\",), (\"£200\",), (\"£300\",)]\n",
    "    input_df = spark.createDataFrame(data, [\"value\"])\n",
    "\n",
    "    expected = 200\n",
    "\n",
    "    # Act\n",
    "    actual = calculate_average(input_df, \"value\")\n",
    "\n",
    "    # Assert\n",
    "    assert actual == expected, f\"Expected {expected} but got {actual}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went as expected, we should have four passing tests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try forcing one to fail by editing the expected output, and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn, trying writing a simple python function, and then write some test cases for it considering the expected and unexpected inputs. The cells below have the \"cell magic\" to create the files for you. \n",
    "For the test, use the Arrange, Act, Assert framework to structure them.\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "- A function to multiply two numbers\n",
    "    - What happens if you input a string?\n",
    "    - Does it work for big numbers? Small numbers? Negative numbers?\n",
    "- A function to test if a number is even\n",
    "    - Does it work as intended and return True for even and False for odd?\n",
    "    - What about negative numbers? 0?\n",
    "- A function to return the longest word in a list of words\n",
    "    - What if the list is empty?\n",
    "    - What if two words have the same length? What would you *want* to happen?\n",
    "    - What if the list was full of numbers?\n",
    "- A function to count the number of vowels in a string\n",
    "    - What is the string is empty?\n",
    "    - What if there are no vowels? Or only vowels?\n",
    "    - What is there's a space in the word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my_new_function.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_my_new_function.py\n",
    "\n",
    "from my_new_function import my_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: running just pytest will run ALL tests it finds, if you only want to run one file put the filename after (including the .py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
