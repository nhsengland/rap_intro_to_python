{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e5a755-bf83-49c0-9b63-b32c92bc68b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac8505a6-1d27-4851-8d2d-16b8c212688d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- **Spark** is an analytics engine that allows queries to be completed by multiple machines (a \"cluster\") in parallel.\n",
    "- This makes it good for dealing with big data\n",
    "- PySpark is an API that enables us to use Spark with python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e211c3-70b0-4932-824b-ed2741ff54b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import PySpark functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a830bba5-52ee-4bf3-81fa-a73710b03bdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "PySpark comes with a library of functions we'll need to use in our code, so we'll import these first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50932cd1-f429-42ed-8dfb-eaa1f0d2c9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, there is a special object called the SparkSession. This is the entry point to Spark's functionality. When you use Databricks, it creates this for you automatically. But outside Databricks, we have to create it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import sql\n",
    "\n",
    "spark = (sql.SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark_intro\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ea296c-c1e6-4df1-ad97-9dd6fd3fa00e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Download Artificial HES data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64847f6-cb06-4846-8950-20dd5e0897ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will use the Artificial NHS Hospital Episode Statistics Accident and Emergency (HES AE) data from 2003 for these examples. The cell below just downloads this data from the public website, and unzips the CSVs inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1cd57a8-06fc-4c58-8b1e-cb71a73f8ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These libraries will help us download the file\n",
    "import zipfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "zip_file_url = \"https://files.digital.nhs.uk/assets/Services/Artificial%20data/Artificial%20HES%20final/artificial_hes_ae_202302_v1_sample.zip\"\n",
    "path_to_downloaded_data = \"inputs/artificial_hes/artificial_hes_ae_202302_v1_sample.zip/artificial_hes_ae_202302_v1_sample/artificial_hes_ae_2122.csv\"\n",
    "\n",
    "filename = Path(zip_file_url).name\n",
    "output_path = f\"inputs/artificial_hes/{filename}\"\n",
    "\n",
    "response = requests.get(zip_file_url, stream=True,timeout=3600)\n",
    "downloaded_zip = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "downloaded_zip.extractall(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame is a tabular data format, stored in memory. We can read a CSV into a DataFrame like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hes = (spark.read\n",
    "    .option('header', 'true')\n",
    "    .csv(path_to_downloaded_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe05f90-b549-4f6e-8ed4-6fc48d97bf30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Displaying data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c8ed02-6bb9-4b35-99fe-91acdc1114d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In Databricks you can use the `display()` function and pass the DataFrame to it. This gives a nice tabular output where you can look at data.\n",
    "\n",
    "It can take a while with a lot of data or intense queries!\n",
    "\n",
    "Outside of Databricks we use `df.show()` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790d17f6-cbc1-442c-bc22-c1f89a4813bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e67698-607e-4a4d-a91c-3103a9a9d3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Manipulating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create a DataFrame object, it automatically has access to a range of functions you can use to transform an manipulate the data. Because these functions are attached to an object, we call them \"methods\".\n",
    "\n",
    "These DataFrame methods, along with the functions we imported using the `from pyspark.sql import functions as F` are what we'll use to work with data in PySpark.\n",
    "\n",
    "We went over quite a few of these last time. Let's recap some of the main ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a20800-9cd3-4f04-a027-64aa6c9f2f07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc3fccb-294d-4c43-9645-537eda0e4dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )             \n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have used the `.select()` method on `df_hes` - this returns a new DataFrame with only the columns we specified.\n",
    "- We stored this new df in a variable called `df_hes_filtered`. The original DataFrame, `df_hes`, is unchanged.\n",
    "- Then we used the `.show()` method to see the results of our query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91abf441-8de8-4999-b379-21c78d72f985",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ba61bf-7c53-4ce1-9546-c99e06d100d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .orderBy(\"ARRIVALDATE\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff8d21d-26c1-4b6e-ac10-08772be10bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's ascending by default. For descending you could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e597a8-3eff-4121-b9eb-bce53be06d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .orderBy( F.desc(\"ARRIVALDATE\") )\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdae826-15b5-4bf2-b8dd-aee599b27c3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Remember: The `F` in `F.desc()` indicates that it's a function from the pyspark.sql.functions library that we imported at the top. So it's not a DataFrame method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd885b74-a5d5-4b8a-9d4e-de3f0beb5a63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1ef5d1-951d-490c-bdd8-1e78dfa5aa8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where( F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "    .orderBy(\"ARRIVALDATE\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we are chaining the DataFrame methods one after another. This is how you build queries in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f18f8b3-598d-4fff-9380-6be913926cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# GROUP BY / COUNT(*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45d4b51-dafe-4765-95db-f4f7fac2b96a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .where(\n",
    "        (F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "        |\n",
    "        (F.col(\"ARRIVALDATE\").isNull())\n",
    "    )\n",
    "    .groupBy(\n",
    "      \"CCG_GP_PRACTICE\"\n",
    "    )\n",
    "    .count()\n",
    "    .orderBy(F.desc('count'))\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Intro",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
