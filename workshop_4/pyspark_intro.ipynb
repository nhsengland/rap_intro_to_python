{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e5a755-bf83-49c0-9b63-b32c92bc68b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Databricks File Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac8505a6-1d27-4851-8d2d-16b8c212688d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In Databricks, if you click \"Workspaces\" on the menu to the left, you'll see a file explorer type interface.\n",
    "\n",
    "There are two main areas:\n",
    "\n",
    "- \"Home\" is your personal area (this is the same as \"Workspace/users/<your_user_name>\")\n",
    "- \"Workspace/Shared\" (where this notebook is), is the shared area that everyone can access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d73ef15-db9d-4ad2-ad39-db23fcb7815d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cloning notebooks in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a79e6a1a-2b9d-41f1-af4e-17eeb8c921b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you're in databricks and want to copy a notebook to your personal file area, click \"File\" from the menu bar at the top, then \"Clone...\", then \"Browse\". Then navigate to your user area and clone it into there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e211c3-70b0-4932-824b-ed2741ff54b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import PySpark functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a830bba5-52ee-4bf3-81fa-a73710b03bdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "PySpark comes with a library of functions we'll need to use in our code, so we'll import these first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50932cd1-f429-42ed-8dfb-eaa1f0d2c9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ea296c-c1e6-4df1-ad97-9dd6fd3fa00e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Download Artificial HES data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64847f6-cb06-4846-8950-20dd5e0897ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Will use the Artificial NHS Hospital Episode Statistics Accident and Emergency (HES AE) data from 2003 for these examples. The cell below just downloads this data from the public website, and unzips it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1cd57a8-06fc-4c58-8b1e-cb71a73f8ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These libraries will help us download the file\n",
    "import zipfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "zip_file_url = \"https://files.digital.nhs.uk/assets/Services/Artificial%20data/Artificial%20HES%20final/artificial_hes_ae_202302_v1_sample.zip\"\n",
    "path_to_downloaded_data = \"data_in/artificial_hes_ae_202302_v1_sample.zip/artificial_hes_ae_202302_v1_sample/artificial_hes_ae_2122.csv\"\n",
    "\n",
    "filename = Path(zip_file_url).name\n",
    "output_path = f\"data_in/{filename}\"\n",
    "\n",
    "response = requests.get(zip_file_url, stream=True,timeout=3600)\n",
    "downloaded_zip = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "downloaded_zip.extractall(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e67698-607e-4a4d-a91c-3103a9a9d3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "the `spark` variable here is known as the SparkSession. It's the entry point to Spark's functionality. Above we used `spark.read` to load data stored in parquet files in an Azure Blob (that's what the 'abfss' part indicates).\n",
    "\n",
    "This returns the data as a DataFrame - basically a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe05f90-b549-4f6e-8ed4-6fc48d97bf30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Displaying data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c8ed02-6bb9-4b35-99fe-91acdc1114d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Just use the display() function and pass the DataFrame to it. Can take a while with a lot of data or intense queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790d17f6-cbc1-442c-bc22-c1f89a4813bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_ec_core_snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c900d25-19c8-42f7-a46c-7008712d9a57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark DataFrame methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b866160a-5b04-42ec-acce-9643a9b7a0a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Most functions you use to manipulate data in PySpark belong to the DataFrame itself. They are functions attached to the DataFrame class. When a function belongs to a class it's known as a method.\n",
    "\n",
    "So everytime you create a DataFrame, it has access to all these dataframe methods. These are what we use to manipulate the data.\n",
    "\n",
    "There's a PySpark equivalent to all the usual suspects in SQL: SELECT, WHERE (filter), GROUP BY, COUNT, etc.\n",
    "\n",
    "To use a DataFrame method, you invoke the name of your dataframe, then a dot, then the name of the method you want to call. E.g.:\n",
    "\n",
    "`df.select()`\n",
    "`df.count()`\n",
    "\n",
    "You chain these methods together to build your query.\n",
    "\n",
    "Here are a few common examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbe5ffe-4179-4522-a178-8e6dded039ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# COUNT(*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46051cef-0d01-4d3d-86b6-362b7a5dc27d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To count all rows, use the `.count()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340fe577-5d74-4f35-a3ab-8b43f84475c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a1ae3c-1263-42fe-bf11-fb112a8c10fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# TOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92963abd-dfe3-47f2-88be-4d94aba6d520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The equivalent of the T-SQL TOP is `.limit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b9aae5-26c4-42bc-b8f3-a095586e9aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The example below creates a new dataframe with the same name as the old one - so we are overwriting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19d6cbe-34ed-420c-a0e9-e97c35fb85c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_top_100 = (df_ec_core_snapshot\n",
    "    .limit(1000)            \n",
    ")\n",
    "\n",
    "df_ec_core_snapshot_top_100.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a20800-9cd3-4f04-a027-64aa6c9f2f07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SELECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3872c14-71b8-45a0-bcd6-d6b46ca487ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use select and pass the names of the columns you want to select.\n",
    "This time we are creating a new dataframe based on the the old one. So the old one still exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc3fccb-294d-4c43-9645-537eda0e4dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )             \n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91abf441-8de8-4999-b379-21c78d72f985",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ORDER BY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd20a5f-b4a4-41db-b31c-96e487739fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`.orderBy()` is pretty straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ba61bf-7c53-4ce1-9546-c99e06d100d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .orderBy(\"Report_Period_Start_Date\")\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff8d21d-26c1-4b6e-ac10-08772be10bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's ascending by default. For descending you could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e597a8-3eff-4121-b9eb-bce53be06d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .orderBy( F.desc(\"Report_Period_Start_Date\") )\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdae826-15b5-4bf2-b8dd-aee599b27c3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `F` in `F.desc()` indicates that it's a function from the pyspark.sql.functions library that we imported at the top. So it's not a DataFrame method.\n",
    "\n",
    "Note that in Spark, order is not guaranteed unless you use `.orderBy()`. It just depends on which workers on the cluster return their results first, which can differ for any number of reasons, machines overheating, network being clogged, cats eating cables in the data centre etc etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd885b74-a5d5-4b8a-9d4e-de3f0beb5a63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# WHERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf63cf3b-9806-4856-adeb-980902e96870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For WHERE clauses you can use `.where()` or `.filter()` - they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1ef5d1-951d-490c-bdd8-1e78dfa5aa8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where( F.col(\"Report_Period_Start_Date\") > \"2023-04-02\")\n",
    "    .orderBy(\"Report_Period_Start_Date\")\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0fe78e-6635-4249-9153-1e52fb9491f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## WHERE col IN () - use `.isin()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cf01be-dd8a-46f4-afee-78d2a9c49986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where(F.col(\"provider_code\").isin([\"RK5\", \"RNS\"]))\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41dbd50d-7645-42ed-b32f-3532efa67000",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## WHERE col NOT IN () - use ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d21c9a-8495-4f92-8e59-fff671a5d1d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where(~F.col(\"provider_code\").isin([\"RK5\", \"RNS\"]))\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1bbb4c-55e6-474e-867f-5302e63c206e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Multiple where clauses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c269432b-09d3-4954-af1e-dcc3c5cfb877",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For AND, you can use `&`, or simply use two `.where()` method calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d1a34f-588c-4b87-853e-430d2005efc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where(F.col(\"Report_Period_Start_Date\") > \"2023-04-02\")\n",
    "    .where(F.col(\"Report_Period_Start_Date\").isNotNull())\n",
    "    .orderBy(\"Report_Period_Start_Date\")\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec65b27-4537-4593-b138-00a010cf38a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"Report_Period_Start_Date\") > \"2023-04-02\")\n",
    "        &\n",
    "        (F.col(\"Report_Period_Start_Date\").isNotNull()) # IS NOT NULL\n",
    "    )\n",
    "    .orderBy(\"Report_Period_Start_Date\")\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155260ed-7c57-4e59-8fae-8a5a3aebf3bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For or use the pipe `|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95979a48-f4b4-4e33-99b3-4991789b584b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"Report_Period_Start_Date\") > \"2023-04-02\")\n",
    "        |\n",
    "        (F.col(\"Report_Period_Start_Date\").isNull()) # IS NULL\n",
    "    )\n",
    "    .orderBy(\"Report_Period_Start_Date\")\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f18f8b3-598d-4fff-9380-6be913926cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# GROUP BY / COUNT(*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fca7b6-0bf7-442a-9e47-5caec3a549f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that the select statement is not necessary here - it gets overrided by the groupBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45d4b51-dafe-4765-95db-f4f7fac2b96a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"Report_Period_Start_Date\") > \"2023-04-02\")\n",
    "        |\n",
    "        (F.col(\"Report_Period_Start_Date\").isNull()) # IS NULL\n",
    "    )\n",
    "    .groupBy(\n",
    "      \"provider_code\"\n",
    "    )\n",
    "    .count()\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b6c2d2-8373-4886-b1a7-b5261e877c27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Other aggregations - use `.agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1875a4a0-ed65-4fd5-9145-eb70cbcfabb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"Report_Period_Start_Date\") > \"2023-04-02\")\n",
    "        |\n",
    "        (F.col(\"Report_Period_Start_Date\").isNull()) # IS NULL\n",
    "    )\n",
    "    .groupBy(\"provider_code\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"EC_Ident\"),\n",
    "        F.min(\"Report_Period_Start_Date\"),\n",
    "        F.max(\"Report_Period_Start_Date\"),\n",
    "        F.sum(\"EC_Ident\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97d97ea-2b22-4ae3-9825-31fb4830fb97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## aliasing columns - `.alias()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f120d60-873a-4688-aaa1-e8e0e12dcf2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### In `.groupBy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c807c8-adba-4b0b-b072-12b0eff366d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"Report_Period_Start_Date\") > \"2023-04-02\")\n",
    "        |\n",
    "        (F.col(\"Report_Period_Start_Date\").isNotNull()) \n",
    "    )\n",
    "    .groupBy(\n",
    "      \"provider_code\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.count(\"EC_Ident\").alias(\"EC_Ident_count\"),\n",
    "        F.countDistinct(\"EC_Ident\").alias(\"EC_Ident_distinct\"),\n",
    "        F.min(\"Report_Period_Start_Date\").alias(\"Min_Report_Period_Start_Date\"),\n",
    "        F.max(\"Report_Period_Start_Date\").alias(\"Max_Report_Period_Start_Date\"),\n",
    "        F.sum(\"EC_Ident\").alias(\"EC_Ident_sum\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049b7662-2419-4995-bce0-166804fed540",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### In `.select()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "978958d0-e45f-4f1f-84ff-b9d93636ad8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To alias in `.select()` use `F.col()` and chain the alias to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4efa56f-d13b-423b-b1fb-12fdce7fd878",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      F.col(\"EC_Ident\").alias(\"EC_Identifier\"),\n",
    "      F.col(\"provider_code\").alias('prov_code'),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0a0a8e-01a9-487f-823d-15d667880091",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Add a new column - `.withColumn()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c67cef2-f5ce-4403-b97f-583cfda19f5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### With the same value for every row - use `.lit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f8a915-36ac-4074-82e5-59aac791d85f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      F.col(\"EC_Ident\").alias(\"EC_Identifier\"),\n",
    "      F.col(\"provider_code\").alias('prov_code'),\n",
    "    )\n",
    "    .withColumn(\"some_flag_column\", F.lit(True))\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4cdbc4-fd16-4711-aea2-9c7ee84f13fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### For a CASE WHEN use `F.when().otherwise()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81aa4bdf-322d-4a0b-8fb1-64eee21547fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "      \"Report_Period_Start_Date\",\n",
    "    )\n",
    "    .withColumn(\"a_new_col\", \n",
    "      F.when(F.col(\"Report_Period_Start_Date\").isNull(), F.lit(\"No date\"))\n",
    "       .when(F.col(\"Report_Period_Start_Date\") > \"2023-05-01\", F.lit(\"After May 1st 2023\"))\n",
    "       .otherwise(F.lit(\"Before May 1st 2023\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c843539-3691-48f7-86f2-5f1869a72c49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating DataFrames manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c2ae3c-a971-4f1c-93ca-4ac9a0627fd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sometimes you might need to create your own DataFrame manually. Maybe to make some reference data that you want to join.\n",
    "\n",
    "To do that, you need to use the `SparkSession`. The `SparkSession` has a method called `.createDataFrame()`. This method takes two parameters:\n",
    "\n",
    "## The data\n",
    "\n",
    "The data takes the form of a list (square brackets) of tuples (round brackets). Each tuple represents a row, and each item in the tuple represents the values for that row across all the columns.\n",
    "\n",
    "## The schema\n",
    "\n",
    "It is possible to specify the column types in the schema, and in some cases you might need to do that. But you can also just give a list of column names, and let Spark figure it out. For this simple example we'll just do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51aa6cc-7524-41c7-a5eb-dd115ead7d62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"RK5\", \"Sherwood Forest Hospitals NHS Foundation Trust\"),\n",
    "    (\"RNS\", \"NORTHAMPTON GENERAL HOSPITAL NHS TRUST\")\n",
    "]\n",
    "\n",
    "schema = [\"provider_code\", \"provider_name\"]\n",
    "\n",
    "df_provider_names = spark.createDataFrame(data, schema)\n",
    "\n",
    "display(df_provider_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "535e31c2-1ef6-4a58-b7b2-219b51fa9c84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c0b8da5-a022-4947-80d2-b34c321d1355",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Just like in SQL, we need three things to do a join:\n",
    "\n",
    "- The name of the **other** DataFrame we're joining to\n",
    "- The columns we want to join **on**\n",
    "- **How** we want to join (left, right, inner, etc.)\n",
    "\n",
    "We just use the `.join()` method and pass these as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e476b7-63de-4ba0-8713-9cbdeb99a007",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "    )\n",
    "    .where(F.col(\"provider_code\").isin([\"RK5\", \"RNS\"]))\n",
    "    .limit(1000)\n",
    "    .join(other=df_provider_names, on=\"provider_code\", how=\"left\")\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbf6a48e-e6d6-46ea-a403-14fe3137438e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Important:** Notice how the `.select()` method does not contain provider_name, but it appears in the results anyway.\n",
    "\n",
    "This is where PySpark differs from SQL a bit. In PySpark, the order of the method calls matters. The join came after the select, so we can't select a column in the other DataFrame, because Spark doesn't know about it yet. \n",
    "\n",
    "So how does provider_name end up in the output?\n",
    "\n",
    "It's because when you left join, all of the other DataFrame's columns are selected by default!\n",
    "\n",
    "If you don't need all the columns, then you can use `.select()` on the other DataFrame when you define it, to ensure you only select the columns you need. Another way would be to put the `.select()` after the `.join()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e7988a0-c7f8-48ef-bda6-fa13a3f8f07a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With `.join()`, you don't need to specify the names of the `other`, `on`, and `how` parameters. As long as you put them in the right order it will still work, and it's conventional to not include the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6932a2b2-92f7-485f-9732-ddbb19b79f8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "    )\n",
    "    .where(F.col(\"provider_code\").isin([\"RK5\", \"RNS\"]))\n",
    "    .limit(1000)\n",
    "    .join(df_provider_names, \"provider_code\", \"left\")\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a7ec29-0b48-4d66-ac16-039102061363",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For an inner join, just change \"left\" to \"inner\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e623a9f8-2d94-4f22-be42-572a9fa6ab39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ec_core_snapshot_filtered = (df_ec_core_snapshot\n",
    "    .select(\n",
    "      \"EC_Ident\",\n",
    "      \"provider_code\",\n",
    "    )\n",
    "    .limit(1000)\n",
    "    .join(df_provider_names, \"provider_code\", \"inner\")\n",
    ")\n",
    "\n",
    "display(df_ec_core_snapshot_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d745dd4-a514-4e32-ac3e-1a691622734f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed13aed5-590b-4311-8e96-f9c6bfc73485",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "A few ideas for practice:\n",
    "\n",
    "- Try making your own queries using the methods above, e.g:\n",
    "  - Could you get the number of rows for provider RWP, grouped by age at arrival, and sorted by age?\n",
    "  - Which provider has the most records in 2023 based on the report period end data?\n",
    "- Try taking a simple SQL script you have used on this data in NCDR and try to recreate it in PySpark here"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Intro",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
