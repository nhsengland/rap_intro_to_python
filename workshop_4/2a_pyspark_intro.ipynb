{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install some extra packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyspark\n",
    "! pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e5a755-bf83-49c0-9b63-b32c92bc68b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Databricks File Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac8505a6-1d27-4851-8d2d-16b8c212688d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In Databricks, if you click \"Workspaces\" on the menu to the left, you'll see a file explorer type interface.\n",
    "\n",
    "There are two main areas:\n",
    "\n",
    "- \"Home\" is your personal area (this is the same as \"Workspace/users/<your_user_name>\")\n",
    "- \"Workspace/Shared\" (where this notebook is), is the shared area that everyone can access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d73ef15-db9d-4ad2-ad39-db23fcb7815d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cloning notebooks in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a79e6a1a-2b9d-41f1-af4e-17eeb8c921b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you're in databricks and want to copy a notebook to your personal file area, click \"File\" from the menu bar at the top, then \"Clone...\", then \"Browse\". Then navigate to your user area and clone it into there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e211c3-70b0-4932-824b-ed2741ff54b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import PySpark functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark comes with a library of functions we'll need to use in our code, so we'll import these first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50932cd1-f429-42ed-8dfb-eaa1f0d2c9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, there is a special object called the SparkSession. This is the entry point to Spark's functionality. When you use Databricks, it creates this for you automatically. But outside Databricks, we have to do it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import sql\n",
    "\n",
    "spark = (sql.SparkSession\n",
    "    .builder\n",
    "    .appName(\"pyspark_intro\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ea296c-c1e6-4df1-ad97-9dd6fd3fa00e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Download Artificial HES data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c64847f6-cb06-4846-8950-20dd5e0897ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will use the Artificial NHS Hospital Episode Statistics Accident and Emergency (HES AE) data from 2003 for these examples. The cell below just downloads this data from the public website, and unzips the CSVs inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1cd57a8-06fc-4c58-8b1e-cb71a73f8ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These libraries will help us download the file\n",
    "import zipfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "zip_file_url = \"https://files.digital.nhs.uk/assets/Services/Artificial%20data/Artificial%20HES%20final/artificial_hes_ae_202302_v1_sample.zip\"\n",
    "path_to_downloaded_data = \"data_in/artificial_hes_ae_202302_v1_sample.zip/artificial_hes_ae_202302_v1_sample/artificial_hes_ae_2122.csv\"\n",
    "\n",
    "filename = Path(zip_file_url).name\n",
    "output_path = f\"data_in/{filename}\"\n",
    "\n",
    "response = requests.get(zip_file_url, stream=True,timeout=3600)\n",
    "downloaded_zip = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "downloaded_zip.extractall(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data from the CSV into a spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hes = (spark.read\n",
    "    .option('header', 'true')\n",
    "    .csv(path_to_downloaded_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e67698-607e-4a4d-a91c-3103a9a9d3cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `spark` variable here is our SparkSession. We used `spark.read.csv` to load data stored in the CSVs we downloaded.\n",
    "\n",
    "In UDAL, data is stored in parquet files, so we'd use spark.read.parquet() instead. And the path would be the location in the Azure Blob of the data you want to read (the path would start with abfss://).\n",
    "\n",
    "`spark.read` returns the data as a DataFrame - basically a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe05f90-b549-4f6e-8ed4-6fc48d97bf30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Displaying data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0c8ed02-6bb9-4b35-99fe-91acdc1114d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In Databricks you can use the `display()` function and pass the DataFrame to it. This gives a nice tabular output where you can look at data.\n",
    "\n",
    "It can take a while with a lot of data or intense queries!\n",
    "\n",
    "Outside of Databricks we use `df.show()` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790d17f6-cbc1-442c-bc22-c1f89a4813bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c900d25-19c8-42f7-a46c-7008712d9a57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PySpark DataFrame methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b866160a-5b04-42ec-acce-9643a9b7a0a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Most functions you use to manipulate data in PySpark belong to the DataFrame itself. They are functions attached to the DataFrame class. When a function belongs to a class it's known as a method.\n",
    "\n",
    "So everytime you create a DataFrame, it has access to all these dataframe methods. These are what we use to manipulate the data.\n",
    "\n",
    "There's a PySpark equivalent to all the usual suspects in SQL: SELECT, WHERE (filter), GROUP BY, COUNT, etc.\n",
    "\n",
    "To use a DataFrame method, you invoke the name of your dataframe, then a dot, then the name of the method you want to call. E.g.:\n",
    "\n",
    "`df.select()`\n",
    "`df.count()`\n",
    "\n",
    "You chain these methods together to build your query.\n",
    "\n",
    "Here are a few common examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbe5ffe-4179-4522-a178-8e6dded039ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# COUNT(*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46051cef-0d01-4d3d-86b6-362b7a5dc27d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To count all rows, use the `.count()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340fe577-5d74-4f35-a3ab-8b43f84475c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used by itself, `.count()` just returns the count as an integer without modifying the DataFrame. This is useful sometimes, as we can capture the count in a variable if we need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hes_count = df_hes.count()\n",
    "\n",
    "print(hes_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a1ae3c-1263-42fe-bf11-fb112a8c10fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# TOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92963abd-dfe3-47f2-88be-4d94aba6d520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The equivalent of the T-SQL TOP is `.limit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b9aae5-26c4-42bc-b8f3-a095586e9aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The example below creates a new dataframe with a different name as the original df_hes one. So we create a new DataFrame called df_hes_top_1000, and the old df_hes one still exists unmodified.\n",
    "\n",
    "If we wanted to overwrite the original one, we'd just use `df_hes = df_hes.limit(1000)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19d6cbe-34ed-420c-a0e9-e97c35fb85c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_top_1000 = (df_hes\n",
    "    .limit(1000)            \n",
    ")\n",
    "\n",
    "df_hes_top_1000.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a20800-9cd3-4f04-a027-64aa6c9f2f07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SELECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3872c14-71b8-45a0-bcd6-d6b46ca487ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the `.select()` method and pass the names of the columns you want to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc3fccb-294d-4c43-9645-537eda0e4dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )             \n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91abf441-8de8-4999-b379-21c78d72f985",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ORDER BY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd20a5f-b4a4-41db-b31c-96e487739fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`.orderBy()` is pretty straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ba61bf-7c53-4ce1-9546-c99e06d100d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .orderBy(\"ARRIVALDATE\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff8d21d-26c1-4b6e-ac10-08772be10bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's ascending by default. For descending you could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e597a8-3eff-4121-b9eb-bce53be06d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .orderBy( F.desc(\"ARRIVALDATE\") )\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdae826-15b5-4bf2-b8dd-aee599b27c3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `F` in `F.desc()` indicates that it's a function from the pyspark.sql.functions library that we imported at the top. So it's not a DataFrame method.\n",
    "\n",
    "Note that in Spark, order is not guaranteed unless you use `.orderBy()`. It just depends on which workers on the cluster return their results first, which can differ for any number of reasons, machines overheating, network being clogged, cats eating cables in the data centre etc etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd885b74-a5d5-4b8a-9d4e-de3f0beb5a63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# WHERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf63cf3b-9806-4856-adeb-980902e96870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For WHERE clauses you can use `.where()` or `.filter()` - they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1ef5d1-951d-490c-bdd8-1e78dfa5aa8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where( F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "    .orderBy(\"ARRIVALDATE\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f0fe78e-6635-4249-9153-1e52fb9491f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## WHERE col IN () - use `.isin()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cf01be-dd8a-46f4-afee-78d2a9c49986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where(F.col(\"CCG_GP_PRACTICE\").isin([\"72Q\", \"91Q\"]))\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41dbd50d-7645-42ed-b32f-3532efa67000",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## WHERE col NOT IN () - use ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the equivelent of the SQL WHERE NOT IN, just use `.isin()` as before, but put a `~` at the start of the expression. This means NOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d21c9a-8495-4f92-8e59-fff671a5d1d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where(~F.col(\"CCG_GP_PRACTICE\").isin([\"72Q\", \"91Q\"]))\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1bbb4c-55e6-474e-867f-5302e63c206e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Multiple where clauses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c269432b-09d3-4954-af1e-dcc3c5cfb877",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For AND, you can use `&`, or simply use two `.where()` method calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d1a34f-588c-4b87-853e-430d2005efc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where(F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "    .where(F.col(\"ARRIVALDATE\").isNotNull())\n",
    "    .orderBy(\"ARRIVALDATE\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec65b27-4537-4593-b138-00a010cf38a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "        &\n",
    "        (F.col(\"ARRIVALDATE\").isNotNull()) # IS NOT NULL\n",
    "    )\n",
    "    .orderBy(\"ARRIVALDATE\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155260ed-7c57-4e59-8fae-8a5a3aebf3bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For OR use the pipe `|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95979a48-f4b4-4e33-99b3-4991789b584b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "        |\n",
    "        (F.col(\"ARRIVALDATE\").isNull()) # IS NULL\n",
    "    )\n",
    "    .orderBy(\"ARRIVALDATE\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f18f8b3-598d-4fff-9380-6be913926cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# GROUP BY / COUNT(*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fca7b6-0bf7-442a-9e47-5caec3a549f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that the select statement is not necessary here - it gets overrided by the groupBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45d4b51-dafe-4765-95db-f4f7fac2b96a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "        |\n",
    "        (F.col(\"ARRIVALDATE\").isNull()) # IS NULL\n",
    "    )\n",
    "    .groupBy(\n",
    "      \"CCG_GP_PRACTICE\"\n",
    "    )\n",
    "    .count()\n",
    "    .orderBy(F.desc('count'))\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b6c2d2-8373-4886-b1a7-b5261e877c27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Other aggregations - use `.agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1875a4a0-ed65-4fd5-9145-eb70cbcfabb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "        |\n",
    "        (F.col(\"ARRIVALDATE\").isNull()) # IS NULL\n",
    "    )\n",
    "    .groupBy(\"CCG_GP_PRACTICE\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"EPIKEY\"),\n",
    "        F.min(\"ARRIVALDATE\"),\n",
    "        F.max(\"ARRIVALDATE\"),\n",
    "        F.sum(\"EPIKEY\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97d97ea-2b22-4ae3-9825-31fb4830fb97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Renaming columns - `.alias()` in chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default column names Spark has given to the aggregate columns are not very pretty. But we can chain `.alias()` onto the aggregate function to rename them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f120d60-873a-4688-aaa1-e8e0e12dcf2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### In `.groupBy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c807c8-adba-4b0b-b072-12b0eff366d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\"\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"ARRIVALDATE\") > \"2021-06-01\")\n",
    "        |\n",
    "        (F.col(\"ARRIVALDATE\").isNotNull()) \n",
    "    )\n",
    "    .groupBy(\n",
    "      \"CCG_GP_PRACTICE\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.count(\"EPIKEY\").alias(\"EPIKEY_count\"),\n",
    "        F.countDistinct(\"EPIKEY\").alias(\"EPIKEY_distinct\"),\n",
    "        F.min(\"ARRIVALDATE\").alias(\"Min_ARRIVALDATE\"),\n",
    "        F.max(\"ARRIVALDATE\").alias(\"Max_ARRIVALDATE\"),\n",
    "        F.sum(\"EPIKEY\").alias(\"EPIKEY_sum\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049b7662-2419-4995-bce0-166804fed540",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### In `.select()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "978958d0-e45f-4f1f-84ff-b9d93636ad8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also us `.alias()` in `.select()`, but to do this, we have to use `F.col()` to name the column and chain `.alias()` to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4efa56f-d13b-423b-b1fb-12fdce7fd878",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      F.col(\"EPIKEY\").alias(\"episode_key\"),\n",
    "      F.col(\"CCG_GP_PRACTICE\").alias('icb_code_of_patients_gp'),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0a0a8e-01a9-487f-823d-15d667880091",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Add a new column - `.withColumn()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.withColumn()` to add new columns to a DataFrame. It takes two parameters - the name of the new column, and the value you want the column to have. \n",
    "\n",
    "To create a column with the same value for every row, use `lit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f8a915-36ac-4074-82e5-59aac791d85f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      F.col(\"EPIKEY\").alias(\"episode_key\"),\n",
    "      F.col(\"CCG_GP_PRACTICE\").alias('icb_code_of_patients_gp'),\n",
    "    )\n",
    "    .withColumn(\"some_flag_column\", F.lit(True))\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can do this within `.select()` too. This way is actually a bit more efficient, so is good practice to do it this way where possible, though in practice it won't make much difference unless you're adding a large number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      F.col(\"EPIKEY\").alias(\"episode_key\"),\n",
    "      F.col(\"CCG_GP_PRACTICE\").alias('icb_code_of_patients_gp'),\n",
    "      F.lit(True).alias(\"some_flag_column\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4cdbc4-fd16-4711-aea2-9c7ee84f13fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### For a CASE WHEN use `F.when().otherwise()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81aa4bdf-322d-4a0b-8fb1-64eee21547fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "      \"ARRIVALDATE\",\n",
    "    )\n",
    "    .withColumn(\"a_new_col\", \n",
    "      F.when(F.col(\"ARRIVALDATE\").isNull(), F.lit(\"No date\"))\n",
    "       .when(F.col(\"ARRIVALDATE\") > \"2021-06-30\", F.lit(\"After June 30th 2021`\"))\n",
    "       .otherwise(F.lit(\"On or before June 30th 2021\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c843539-3691-48f7-86f2-5f1869a72c49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creating DataFrames manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c2ae3c-a971-4f1c-93ca-4ac9a0627fd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sometimes you might need to create your own DataFrame manually. Maybe to make some reference data that you want to join.\n",
    "\n",
    "To do that, you need to use the `SparkSession`. The `SparkSession` has a method called `.createDataFrame()`. This method takes two parameters:\n",
    "\n",
    "## The data\n",
    "\n",
    "The data takes the form of a list (square brackets) of tuples (round brackets). Each tuple represents a row, and each item in the tuple represents the values for that row across all the columns.\n",
    "\n",
    "## The schema\n",
    "\n",
    "It is possible to specify the column types in the schema, and in some cases you might need to do that. But you can also just give a list of column names, and let Spark figure it out. For this simple example we'll just do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51aa6cc-7524-41c7-a5eb-dd115ead7d62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"72Q\", \"NHS SOUTH EAST LONDON ICB\"),\n",
    "    (\"91Q\", \"NHS KENT AND MEDWAY ICB\")\n",
    "]\n",
    "\n",
    "schema = [\"CCG_GP_PRACTICE\", \"icb_name\"]\n",
    "\n",
    "df_icb_names = spark.createDataFrame(data, schema)\n",
    "\n",
    "df_icb_names.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "535e31c2-1ef6-4a58-b7b2-219b51fa9c84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c0b8da5-a022-4947-80d2-b34c321d1355",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Just like in SQL, we need three things to do a join:\n",
    "\n",
    "- The name of the **other** DataFrame we're joining to\n",
    "- The column/s we want to join **on**\n",
    "- **How** we want to join (left, right, inner, etc.)\n",
    "\n",
    "We just use the `.join()` method and pass these as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e476b7-63de-4ba0-8713-9cbdeb99a007",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "    )\n",
    "    .where(F.col(\"CCG_GP_PRACTICE\").isin([\"72Q\", \"91Q\"]))\n",
    "    .limit(1000)\n",
    "    .join(other=df_icb_names, on=\"CCG_GP_PRACTICE\", how=\"left\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbf6a48e-e6d6-46ea-a403-14fe3137438e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Important:** Notice how the `.select()` method does not contain icb_name, but it appears in the results anyway.\n",
    "\n",
    "This is where PySpark differs from SQL a bit. In PySpark, the order of the method calls matters. The join came after the select, so we can't select a column in the other DataFrame, because Spark doesn't know about it yet. \n",
    "\n",
    "So how does provider_name end up in the output?\n",
    "\n",
    "It's because when you left join, all of the other DataFrame's columns are selected by default!\n",
    "\n",
    "If you don't need all the columns, then you can use `.select()` on the other DataFrame upstream when you define it, to ensure you only select the columns you need. Another way would be to put the `.select()` after the `.join()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e7988a0-c7f8-48ef-bda6-fa13a3f8f07a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With `.join()`, you don't need to specify the names of the `other`, `on`, and `how` parameters. As long as you put them in the right order it will still work, and it's conventional to not include the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6932a2b2-92f7-485f-9732-ddbb19b79f8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "    )\n",
    "    .where(F.col(\"CCG_GP_PRACTICE\").isin([\"72Q\", \"91Q\"]))\n",
    "    .join(df_icb_names, \"CCG_GP_PRACTICE\", \"left\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an inner join, just change \"left\" to \"inner\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e623a9f8-2d94-4f22-be42-572a9fa6ab39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_hes_filtered = (df_hes\n",
    "    .select(\n",
    "      \"EPIKEY\",\n",
    "      \"CCG_GP_PRACTICE\",\n",
    "    )\n",
    "    .join(df_icb_names, \"CCG_GP_PRACTICE\", \"inner\")\n",
    ")\n",
    "\n",
    "df_hes_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNION - .union()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a union, we can use the `.union()` method. Notice how the HES financial years are all on separate CSV files.\n",
    "\n",
    "Let's say we want to do some analysis on two financial years at once.\n",
    "\n",
    "First, we'd load both of the CSVs we need into DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_18_19_data = \"data_in/artificial_hes_ae_202302_v1_sample.zip/artificial_hes_ae_202302_v1_sample/artificial_hes_ae_1819.csv\"\n",
    "path_to_19_20_data = \"data_in/artificial_hes_ae_202302_v1_sample.zip/artificial_hes_ae_202302_v1_sample/artificial_hes_ae_1920.csv\"\n",
    "\n",
    "df_hes_18_19 = (spark.read\n",
    "    .option('header', 'true')\n",
    "    .csv(path_to_18_19_data)\n",
    ")\n",
    "\n",
    "df_hes_19_20 = (spark.read\n",
    "    .option('header', 'true')\n",
    "    .csv(path_to_19_20_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that we have our two dfs, let's union them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hes_all = (df_hes_18_19\n",
    "    .union(df_hes_19_20)              \n",
    ")\n",
    "\n",
    "df_hes_all.select(\"FYEAR\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a union, the dataframes need to have the same number of columns and compatible column types. For example, if we try to union the small DF of ICB names we created above, this won't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hes_all = (df_hes_18_19\n",
    "    .union(df_icb_names)              \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d745dd4-a514-4e32-ac3e-1a691622734f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed13aed5-590b-4311-8e96-f9c6bfc73485",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "If you want to get some practice in, try some of these:\n",
    "\n",
    "- Try making your own queries using the methods above, e.g:\n",
    "  - Could you get the number of episodes for the CCG_GP_PRACTICE 029, grouped by arrivaldata, and sorted by the number of episodes (EPIKEY)?\n",
    "  - Which CCG_GP_PRACTICE has the most episodes in the data, in the first quarter of the financial year according to arrival date?\n",
    "  - Are there any duplicate episode keys (EPIKEY) in the data? (hint: try `.distinct()`...)\n",
    "- could you recreate the following SQL query in PySpark?:\n",
    "\n",
    "```SQL\n",
    "select\n",
    "  AEARRIVALMODE AE_arrival_mode,\n",
    "  count(EPIKEY) episode_count\n",
    "from\n",
    "  hes.ae_2122\n",
    "where\n",
    "  SEX = 1\n",
    "group by\n",
    "  AEARRIVALMODE\n",
    "order by\n",
    "  count(epikey) desc\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Intro",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
